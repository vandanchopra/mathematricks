{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "from gym import spaces\n",
    "# from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "import tqdm\n",
    "from typing import List\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = 'cpu'\n",
    "if device == 'cpu':\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cpu':\n",
    "  device = torch.device('mps' if torch.has_mps else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Handling\n",
    "def load_stock_data(folder_path):\n",
    "    \"\"\"Load stock data from individual csv files and returns dict of dataframes\"\"\"\n",
    "    all_data = {}\n",
    "    for filename in os.listdir(folder_path)[:10]: # only load first 10 files\n",
    "        if filename.endswith(\".csv\"):\n",
    "            symbol = filename[:-4] # remove .csv\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            all_data[symbol] = pd.read_csv(filepath, index_col='datetime', parse_dates=True)\n",
    "    return all_data\n",
    "\n",
    "def calculate_forward_returns(df, days_forward=1):\n",
    "  \"\"\"Calculate simple forward returns using future open prices.\"\"\"\n",
    "  df['Forward_Return'] = df['open'].shift(-days_forward) / df['open'] - 1\n",
    "  df.dropna(inplace=True) # drop rows with no future data\n",
    "  return df\n",
    "\n",
    "def create_dataset(dataframes, start_date, end_date, days_forward=1):\n",
    "    \"\"\" Create one pandas dataframe using a dictionary of dataframes.\"\"\"\n",
    "    combined_df = pd.concat([df.assign(symbol=symbol) for symbol, df in dataframes.items()], ignore_index=False)\n",
    "    combined_df = combined_df.reset_index().set_index('datetime')\n",
    "\n",
    "    # Convert columns to numeric, handling non-numeric values\n",
    "    cols_to_convert = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in cols_to_convert:\n",
    "      combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "      \n",
    "    # Drop cols not in cols_to_convert\n",
    "    combined_df = combined_df[cols_to_convert + ['symbol']]\n",
    "\n",
    "    combined_df = combined_df.dropna()\n",
    "    combined_df = combined_df.sort_index()  # Ensure the DatetimeIndex is sorted\n",
    "    combined_df = combined_df.loc[start_date:end_date]\n",
    "    combined_df = combined_df.groupby('symbol').apply(lambda x: calculate_forward_returns(x, days_forward=days_forward)).reset_index(level=0, drop=True)\n",
    "    # print({'combined_df':combined_df})\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    combined_df.set_index(['datetime', 'symbol'], inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "# 2. Sentiment Analysis (Placeholder)\n",
    "def get_sentiment_scores(text_data):\n",
    "    \"\"\"Placeholder - Integrate with FinGPT later\"\"\"\n",
    "    # This should return a sentiment score\n",
    "    return np.random.uniform(-1, 1, len(text_data))\n",
    "\n",
    "# 3. Trading Environment (using finrl's StockTradingEnv)\n",
    "# Setup the environment using FinRL library's stock trading environment\n",
    "def create_finrl_env(all_stocks_data, start_date, end_date, initial_amount=1000000):\n",
    "    dataset = create_dataset(all_stocks_data, start_date, end_date).reset_index()\n",
    "    dataset.rename(columns={\"symbol\": \"tic\"}, inplace=True)\n",
    "    \n",
    "    tech_indicators = ['open', 'high', 'low', 'close', 'volume']\n",
    "    env_kwargs = {\n",
    "        'df': dataset,\n",
    "        'stock_dim': len(all_stocks_data),\n",
    "        'hmax': 100,\n",
    "        'initial_amount': initial_amount,\n",
    "        'buy_cost_pct': 0.001,\n",
    "        'sell_cost_pct': 0.001,\n",
    "        'reward_scaling': 1e-4,\n",
    "        'print_verbosity': 5,\n",
    "        'num_stock_shares': [100] * len(all_stocks_data),\n",
    "        'tech_indicator_list': tech_indicators,\n",
    "        # Ensure these two match your environmentâ€™s expectations\n",
    "        'state_space': len(tech_indicators) * len(all_stocks_data),\n",
    "        'action_space': len(all_stocks_data),\n",
    "    }\n",
    "\n",
    "    env = StockTradingEnv(**env_kwargs)\n",
    "    return env\n",
    "  \n",
    "def minimal_finrl_test(all_stocks_data):\n",
    "  \"\"\"Test that the environment can be initialized\"\"\"\n",
    "  start_date_test = '2021-01-01'\n",
    "  end_date_test = '2021-12-31'\n",
    "  env_test = create_finrl_env(all_stocks_data, start_date_test, end_date_test)\n",
    "  print(\"FinRL StockTradingEnv test: Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/vandanchopra/Vandan_Personal_Folder/CODE_STUFF/Projects/mathematricks/db/data/ibkr/1d\"  # Replace with your data folder path\n",
    "all_stocks_data = load_stock_data(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. HLC (High-Level Controller)\n",
    "class HLC_PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(HLC_PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class HLC_ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(HLC_ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def ppo_update(hlc_policy, hlc_value, optimizer_policy, optimizer_value, states, actions, returns, advantages, clip_param=0.2, entropy_beta=0.01):\n",
    "    \"\"\"Standard PPO update step\"\"\"\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.float32).to(device)\n",
    "    returns = torch.tensor(np.array(returns), dtype=torch.float32).to(device)\n",
    "    advantages = torch.tensor(np.array(advantages), dtype=torch.float32).to(device)\n",
    "    #calculate new policy\n",
    "    new_actions = hlc_policy(states)\n",
    "    new_action_probs = torch.sigmoid(new_actions)\n",
    "\n",
    "    old_action_probs = torch.sigmoid(actions)\n",
    "\n",
    "    ratio = torch.exp(torch.log(new_action_probs) - torch.log(old_action_probs))\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_param, 1 + clip_param)\n",
    "    surrogate_objective = torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "    #entropy regularization - prevent policy from becoming deterministic\n",
    "    entropy = -(new_action_probs * torch.log(new_action_probs + 1e-8)).mean() # add a small number to avoid log of 0\n",
    "\n",
    "    #policy gradient update\n",
    "    policy_loss = - (surrogate_objective - entropy_beta * entropy)\n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_policy.step()\n",
    "\n",
    "    #calculate value loss\n",
    "    values = hlc_value(states).squeeze()\n",
    "    value_loss = ((values - returns)**2).mean()\n",
    "    optimizer_value.zero_grad()\n",
    "    value_loss.backward()\n",
    "    optimizer_value.step()\n",
    "\n",
    "def create_advantage_estimator(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
    "  \"\"\"Generalized advantage estimation\"\"\"\n",
    "  advantages = []\n",
    "  advantage = 0\n",
    "  for i in reversed(range(len(rewards))):\n",
    "    delta = rewards[i] + gamma * values[i+1] - values[i] if i < len(rewards) - 1 else rewards[i] - values[i]\n",
    "    advantage = delta + gamma * gae_lambda * advantage\n",
    "    advantages.insert(0, advantage) # insert at beginning so we have correct order for training\n",
    "  return advantages\n",
    "\n",
    "# 5. LLC (Low-Level Controller)\n",
    "class LLC_ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LLC_ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class LLC_CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(LLC_CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def ddpg_update(actor, critic, target_actor, target_critic, optimizer_actor, optimizer_critic, replay_buffer, batch_size, discount_factor=0.99, tau=0.005):\n",
    "    \"\"\"Standard DDPG update step\"\"\"\n",
    "    if len(replay_buffer) < batch_size: return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.float32).to(device)\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(np.array(dones), dtype=torch.float32).to(device)\n",
    "\n",
    "    target_actions = target_actor(next_states)\n",
    "    target_q_values = target_critic(next_states, target_actions).squeeze()\n",
    "    expected_q_values = rewards + (discount_factor * target_q_values * (1 - dones))\n",
    "\n",
    "    #critic loss\n",
    "    q_values = critic(states, actions).squeeze()\n",
    "    critic_loss = ((q_values - expected_q_values)**2).mean()\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "\n",
    "    #actor loss\n",
    "    policy_actions = actor(states)\n",
    "    actor_loss = -critic(states, policy_actions).mean()\n",
    "    optimizer_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "\n",
    "    #update target networks\n",
    "    for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "      target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "      target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "# 6. HRT Agent\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer.\"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "      batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "      states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in batch])\n",
    "      return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class HRTAgent:\n",
    "    def __init__(self, env, hlc_config, llc_config, device):\n",
    "        # Create High Level Controller (HLC)\n",
    "        self.num_stocks = env.stock_dim\n",
    "        self.hlc_policy = HLC_PolicyNetwork(hlc_config[\"input_size\"], hlc_config[\"hidden_size\"], hlc_config[\"output_size\"]).to(device)\n",
    "        self.hlc_value = HLC_ValueNetwork(hlc_config[\"input_size\"], hlc_config[\"hidden_size\"]).to(device)\n",
    "        self.hlc_optimizer_policy = optim.Adam(self.hlc_policy.parameters(), lr=hlc_config[\"learning_rate_policy\"])\n",
    "        self.hlc_optimizer_value = optim.Adam(self.hlc_value.parameters(), lr=hlc_config[\"learning_rate_value\"])\n",
    "        # Create Low Level Controller (LLC)\n",
    "        self.llc_actor = LLC_ActorNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_critic = LLC_CriticNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_target_actor = LLC_ActorNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_target_critic = LLC_CriticNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_optimizer_actor = optim.Adam(self.llc_actor.parameters(), lr=llc_config[\"learning_rate_actor\"])\n",
    "        self.llc_optimizer_critic = optim.Adam(self.llc_critic.parameters(), lr=llc_config[\"learning_rate_critic\"])\n",
    "        self.llc_replay_buffer = ReplayBuffer(llc_config[\"replay_buffer_size\"])\n",
    "        # Initialize Target Networks\n",
    "        self.llc_target_actor.load_state_dict(self.llc_actor.state_dict())\n",
    "        self.llc_target_critic.load_state_dict(self.llc_critic.state_dict())\n",
    "\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, state, exploration_noise=None):\n",
    "      with torch.no_grad():\n",
    "        hlc_state = torch.tensor(state[\"hlc_state\"], dtype=torch.float32).to(self.device)\n",
    "        hlc_action = self.hlc_policy(hlc_state).cpu().numpy()\n",
    "        llc_actions = []\n",
    "        for i in range(self.num_stocks):\n",
    "          if hlc_action[i] > 0.5: #buy action\n",
    "            llc_state = torch.tensor(state[\"llc_state\"][i], dtype=torch.float32).to(self.device)\n",
    "            action = self.llc_actor(llc_state)\n",
    "            if exploration_noise is not None:\n",
    "              noise = torch.normal(torch.zeros(action.shape), exploration_noise).to(self.device)\n",
    "              action = (action + noise).clamp(-1, 1)\n",
    "            llc_actions.append(action.cpu().numpy())\n",
    "          elif hlc_action[i] < -0.5: #sell action\n",
    "              llc_state = torch.tensor(state[\"llc_state\"][i], dtype=torch.float32).to(self.device)\n",
    "              action = self.llc_actor(llc_state)\n",
    "              if exploration_noise is not None:\n",
    "                noise = torch.normal(torch.zeros(action.shape), exploration_noise).to(self.device)\n",
    "                action = (action + noise).clamp(-1, 1)\n",
    "              llc_actions.append(action.cpu().numpy())\n",
    "          else: # hold action\n",
    "              llc_actions.append(np.array(0))\n",
    "\n",
    "        return hlc_action, llc_actions\n",
    "\n",
    "    def calculate_hlc_reward(self, last_state, next_state, hlc_action, llc_reward, at, price_changes):\n",
    "      \"\"\"Calculate alignment and overall reward\"\"\"\n",
    "      #calculate alignment reward\n",
    "      alignment_rewards = []\n",
    "      for i in range(self.num_stocks):\n",
    "        action_i = 1 if hlc_action[i] > 0.5 else -1 if hlc_action[i] < -0.5 else 0\n",
    "        price_change_sign = np.sign(price_changes[i])\n",
    "        alignment_reward = np.sign(action_i) * price_change_sign if action_i != 0 else 0\n",
    "        alignment_rewards.append(alignment_reward)\n",
    "      alignment_rewards = np.array(alignment_rewards)\n",
    "\n",
    "      return at * np.sum(alignment_rewards) + (1 - at) * llc_reward\n",
    "\n",
    "    def phased_alternating_train(self, num_episodes, env, training_config, writer, at_initial_value = 1, at_decay = 0.001):\n",
    "      \"\"\" Train using the phased alternating algorithm.\"\"\"\n",
    "\n",
    "      self.current_step = 0\n",
    "      for episode in tqdm.tqdm(range(num_episodes), desc=\"Training Progress\", colour=\"green\"):\n",
    "          at = at_initial_value * np.exp(-at_decay * episode)\n",
    "          # Phase 1: HLC Training\n",
    "          if episode < training_config[\"hlc_episodes\"]:\n",
    "              states, actions, rewards, log_probs, values = self.train_hlc(env, training_config, writer, at, hlc_only=True)\n",
    "              for step, reward in enumerate(rewards):\n",
    "                writer.add_scalar('Rewards/HLC_only', reward, self.current_step + step)\n",
    "              self.current_step += len(states)\n",
    "          # Phase 2: LLC Training\n",
    "          elif episode >= training_config[\"hlc_episodes\"] and episode < training_config[\"llc_episodes\"]:\n",
    "              self.train_llc(env, training_config, writer)\n",
    "          # Phase 3: Alternating Training\n",
    "          else:\n",
    "            states, actions, rewards, log_probs, values = self.train_hlc(env, training_config, writer, at, hlc_only=False)\n",
    "            for step, reward in enumerate(rewards):\n",
    "              writer.add_scalar('Rewards/Combined', reward, self.current_step + step)\n",
    "            self.current_step += len(states)\n",
    "            self.train_llc(env, training_config, writer)\n",
    "\n",
    "    def train_hlc(self, env, training_config, writer, at, hlc_only=False):\n",
    "      \"\"\"Train the HLC network\"\"\"\n",
    "      states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "\n",
    "      state = env.reset()\n",
    "      done = False\n",
    "      t = 0\n",
    "      while not done:\n",
    "        hlc_state = state[\"hlc_state\"]\n",
    "        llc_state = state[\"llc_state\"]\n",
    "        hlc_action, llc_actions = self.select_action(state, exploration_noise=None)\n",
    "        next_state, reward, done, _ = env.step(llc_actions)\n",
    "\n",
    "        price_changes = []\n",
    "        if t > 0: # calculate if there were price changes\n",
    "            for i in range(self.num_stocks):\n",
    "                price_changes.append(next_state[\"stock_prices\"][i] - state[\"stock_prices\"][i])\n",
    "        else: #set to 0 on first iteration\n",
    "            price_changes = np.zeros(self.num_stocks)\n",
    "\n",
    "        # calculate reward\n",
    "        if hlc_only:\n",
    "          r = self.calculate_hlc_reward(state, next_state, hlc_action, 0, at, price_changes)\n",
    "        else:\n",
    "          r = self.calculate_hlc_reward(state, next_state, hlc_action, reward, at, price_changes)\n",
    "        states.append(hlc_state)\n",
    "        actions.append(hlc_action)\n",
    "        values.append(self.hlc_value(torch.tensor(np.array(hlc_state), dtype=torch.float32).to(self.device)).detach().cpu().numpy())\n",
    "        rewards.append(r)\n",
    "        state = next_state\n",
    "        t += 1\n",
    "\n",
    "      values.append(self.hlc_value(torch.tensor(np.array(next_state[\"hlc_state\"]), dtype=torch.float32).to(self.device)).detach().cpu().numpy())\n",
    "      advantages = create_advantage_estimator(rewards, values, gamma=training_config[\"gamma\"], gae_lambda=training_config[\"gae_lambda\"])\n",
    "      for _ in range(training_config[\"ppo_epochs\"]):\n",
    "        ppo_update(self.hlc_policy, self.hlc_value, self.hlc_optimizer_policy, self.hlc_optimizer_value, states, actions, rewards, advantages)\n",
    "\n",
    "      return states, actions, rewards, log_probs, values\n",
    "\n",
    "    def train_llc(self, env, training_config, writer):\n",
    "        \"\"\"Train the LLC network\"\"\"\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "          hlc_state = state[\"hlc_state\"]\n",
    "          llc_state = state[\"llc_state\"]\n",
    "          hlc_action, llc_actions = self.select_action(state, exploration_noise=training_config[\"exploration_noise\"])\n",
    "          next_state, reward, done, _ = env.step(llc_actions)\n",
    "\n",
    "          # Store in replay buffer\n",
    "          for i in range(self.num_stocks):\n",
    "            if hlc_action[i] > 0.5 or hlc_action[i] < -0.5: #only add to buffer if we take action\n",
    "              self.llc_replay_buffer.push(llc_state[i], llc_actions[i], reward, next_state[\"llc_state\"][i], done)\n",
    "\n",
    "          ddpg_update(self.llc_actor, self.llc_critic, self.llc_target_actor, self.llc_target_critic, self.llc_optimizer_actor, self.llc_optimizer_critic, self.llc_replay_buffer, training_config[\"batch_size\"], discount_factor=training_config[\"gamma\"], tau=training_config[\"tau\"])\n",
    "\n",
    "          state = next_state\n",
    "\n",
    "        writer.add_scalar('Rewards/LLC', reward, self.current_step)\n",
    "\n",
    "# 7. Training and Testing\n",
    "def train_hrt_agent(agent, env, num_episodes, training_config, writer):\n",
    "  \"\"\"Train the HRT agent\"\"\"\n",
    "  agent.phased_alternating_train(num_episodes, env, training_config, writer)\n",
    "  return agent\n",
    "\n",
    "def test_hrt_agent(agent, env, num_episodes, writer):\n",
    "  \"\"\" Test the HRT agent and calculate performance metrics.\"\"\"\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  total_reward = 0\n",
    "  trades = 0\n",
    "  while not done:\n",
    "    hlc_action, llc_actions = agent.select_action(state, exploration_noise=None)\n",
    "    next_state, reward, done, _ = env.step(llc_actions)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    trades +=1\n",
    "  writer.add_scalar('Results/Test_TotalReward', total_reward, 1)\n",
    "  writer.add_scalar('Results/Test_NumTrades', trades, 1)\n",
    "  print(f\"Test Result - Total reward: {total_reward}, Trades taken {trades}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/_rd9vn610y36nmcvbvp1xr480000gn/T/ipykernel_64594/3484107893.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined_df = combined_df.groupby('symbol').apply(lambda x: calculate_forward_returns(x, days_forward=days_forward)).reset_index(level=0, drop=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m all_stocks_data \u001b[38;5;241m=\u001b[39m load_stock_data(data_folder)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Test the minimal stock env\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mminimal_finrl_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_stocks_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m start_date_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m end_date_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2019-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m, in \u001b[0;36mminimal_finrl_test\u001b[0;34m(all_stocks_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m start_date_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     75\u001b[0m end_date_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 76\u001b[0m env_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_finrl_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_stocks_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinRL StockTradingEnv test: Environment created successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 69\u001b[0m, in \u001b[0;36mcreate_finrl_env\u001b[0;34m(all_stocks_data, start_date, end_date, initial_amount)\u001b[0m\n\u001b[1;32m     52\u001b[0m tech_indicators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m env_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(all_stocks_data),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(all_stocks_data),\n\u001b[1;32m     67\u001b[0m }\n\u001b[0;32m---> 69\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mStockTradingEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/Vandan_Personal_Folder/CODE_STUFF/Projects/venvs/mathematricks_venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:76\u001b[0m, in \u001b[0;36mStockTradingEnv.__init__\u001b[0;34m(self, df, stock_dim, hmax, initial_amount, num_stock_shares, buy_cost_pct, sell_cost_pct, reward_scaling, state_space, action_space, tech_indicator_list, turbulence_threshold, risk_indicator_col, make_plots, print_verbosity, day, initial, previous_state, model_name, mode, iteration)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m=\u001b[39m iteration\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# initalize state\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initiate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# initialize reward\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Vandan_Personal_Folder/CODE_STUFF/Projects/venvs/mathematricks_venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:409\u001b[0m, in \u001b[0;36mStockTradingEnv._initiate_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# For Initial State\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mtic\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;66;03m# for multiple stock\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    408\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_amount]\n\u001b[0;32m--> 409\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stock_shares\n\u001b[1;32m    411\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    412\u001b[0m                 (\n\u001b[1;32m    413\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[tech]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    414\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m tech \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtech_indicator_list\n\u001b[1;32m    415\u001b[0m                 ),\n\u001b[1;32m    416\u001b[0m                 [],\n\u001b[1;32m    417\u001b[0m             )\n\u001b[1;32m    418\u001b[0m         )  \u001b[38;5;66;03m# append initial stocks_share to initial state, instead of all zero\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;66;03m# for single stock\u001b[39;00m\n\u001b[1;32m    421\u001b[0m         state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    422\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_amount]\n\u001b[1;32m    423\u001b[0m             \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclose]\n\u001b[1;32m    424\u001b[0m             \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim\n\u001b[1;32m    425\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[tech]] \u001b[38;5;28;01mfor\u001b[39;00m tech \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtech_indicator_list), [])\n\u001b[1;32m    426\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "# if __name__ == \"__main__\":\n",
    "# Load Data and setup the environment\n",
    "data_folder = \"/Users/vandanchopra/Vandan_Personal_Folder/CODE_STUFF/Projects/mathematricks/db/data/ibkr/1d\"  # Replace with your data folder path\n",
    "all_stocks_data = load_stock_data(data_folder)\n",
    "\n",
    "# Test the minimal stock env\n",
    "minimal_finrl_test(all_stocks_data)\n",
    "\n",
    "start_date_train = '2015-01-01'\n",
    "end_date_train = '2019-12-31'\n",
    "start_date_test = '2021-01-01'\n",
    "end_date_test = '2021-12-31'\n",
    "env_train = create_finrl_env(all_stocks_data, start_date_train, end_date_train)\n",
    "env_test = create_finrl_env(all_stocks_data, start_date_test, end_date_test)\n",
    "\n",
    "# Configuration\n",
    "hlc_config = {\n",
    "  \"input_size\": env_train.observation_space.shape[0],  # Use the observation space of your specific environment.\n",
    "  \"hidden_size\": 128,\n",
    "  \"output_size\": len(all_stocks_data), # buy/sell/hold for each stock\n",
    "  \"learning_rate_policy\": 3e-4,\n",
    "  \"learning_rate_value\": 1e-3\n",
    "}\n",
    "llc_config = {\n",
    "  \"input_size\": env_train.observation_space.shape[0], # observation for each stock\n",
    "  \"hidden_size\": 128,\n",
    "  \"output_size\": 1,  # trading volume for each stock\n",
    "  \"learning_rate_actor\": 1e-3,\n",
    "  \"learning_rate_critic\": 1e-3,\n",
    "  \"replay_buffer_size\": 200000\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "  \"hlc_episodes\": 10,\n",
    "  \"llc_episodes\": 20,\n",
    "  \"ppo_epochs\": 5,\n",
    "  \"exploration_noise\": 0.1,\n",
    "  \"batch_size\": 256,\n",
    "  \"gamma\": 0.99,\n",
    "  \"gae_lambda\": 0.95,\n",
    "  \"tau\": 0.005\n",
    "}\n",
    "\n",
    "num_training_episodes = 2 # Adjust as needed\n",
    "writer = SummaryWriter()\n",
    "# Create and Train HRT Agent\n",
    "hrt_agent = HRTAgent(env_train, hlc_config, llc_config, device)\n",
    "trained_agent = train_hrt_agent(hrt_agent, env_train, num_training_episodes, training_config, writer)\n",
    "# Test the agent\n",
    "test_hrt_agent(trained_agent, env_test, num_training_episodes, writer)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathematricks_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
