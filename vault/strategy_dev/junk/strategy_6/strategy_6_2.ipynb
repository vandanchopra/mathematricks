{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "from gym import spaces\n",
    "import tqdm\n",
    "from typing import List\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = 'cpu'\n",
    "if device == 'cpu':\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cpu':\n",
    "  device = torch.device('mps' if torch.has_mps else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gym\n",
    "from gym import spaces\n",
    "import tqdm\n",
    "from typing import List\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 1. Data Handling\n",
    "def load_stock_data(folder_path):\n",
    "    \"\"\"Load stock data from individual csv files and returns dict of dataframes\"\"\"\n",
    "    all_data = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            symbol = filename[:-4] # remove .csv\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            all_data[symbol] = pd.read_csv(filepath, index_col='datetime', parse_dates=True)\n",
    "    return all_data\n",
    "\n",
    "def calculate_forward_returns(df, days_forward=1):\n",
    "  \"\"\"Calculate simple forward returns using future open prices.\"\"\"\n",
    "  df['Forward_Return'] = df['open'].shift(-days_forward) / df['open'] - 1\n",
    "  df.dropna(inplace=True) # drop rows with no future data\n",
    "  return df\n",
    "\n",
    "def create_dataset(dataframes, start_date, end_date, days_forward=1):\n",
    "    \"\"\" Create one pandas dataframe using a dictionary of dataframes.\"\"\"\n",
    "    combined_df = pd.concat([df.assign(symbol=symbol) for symbol, df in dataframes.items()], ignore_index=False)\n",
    "    combined_df = combined_df.reset_index().set_index('datetime')\n",
    "\n",
    "    # Convert columns to numeric, handling non-numeric values\n",
    "    cols_to_convert = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in cols_to_convert:\n",
    "      combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "      \n",
    "    # Drop cols not in cols_to_convert\n",
    "    combined_df = combined_df[cols_to_convert + ['symbol']]\n",
    "\n",
    "    combined_df = combined_df.dropna()\n",
    "    combined_df = combined_df.loc[start_date:end_date]\n",
    "    combined_df = combined_df.groupby('symbol').apply(lambda x: calculate_forward_returns(x, days_forward=days_forward))\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    combined_df.set_index(['datetime', 'symbol'], inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "# 2. Sentiment Analysis (Placeholder)\n",
    "def get_sentiment_scores(text_data):\n",
    "    \"\"\"Placeholder - Integrate with FinGPT later\"\"\"\n",
    "    # This should return a sentiment score\n",
    "    return np.random.uniform(-1, 1, len(text_data))\n",
    "\n",
    "# 3. Trading Environment (Custom)\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"Custom stock trading environment.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframes, start_date, end_date, initial_amount=1000000, hmax=100, buy_cost_pct=0.001, sell_cost_pct=0.001, reward_scaling = 1e-4):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.all_stocks_data = dataframes\n",
    "        self.initial_amount = initial_amount\n",
    "        self.hmax = hmax\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.tickers = list(self.all_stocks_data.keys())\n",
    "        self.num_stocks = len(self.tickers)\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.data = create_dataset(self.all_stocks_data, start_date, end_date)\n",
    "\n",
    "        # Define action space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.num_stocks,), dtype=np.float32)\n",
    "        # Define observation space: this requires both historical and real-time data\n",
    "        # state = [stock_prices, holdings, balance, forward_returns, sentiment_scores]\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"hlc_state\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.num_stocks * 2,), dtype=np.float32), # for hlc, forward returns, sentiment scores\n",
    "                \"llc_state\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.num_stocks, 3), dtype=np.float32), # for each stock, price, holding, balance\n",
    "                \"stock_prices\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.num_stocks,), dtype=np.float32), # current price of each stock\n",
    "\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data.index.unique()) # number of timesteps is max index\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "      \"\"\"Reset the environment to the beginning of a new episode.\"\"\"\n",
    "      super().reset(seed=seed)\n",
    "      self.current_step = 0\n",
    "      self.portfolio_value = self.initial_amount\n",
    "      self.holdings = np.zeros(self.num_stocks) #Initialise holdings to 0\n",
    "      self.balance = self.initial_amount\n",
    "      # Initialise with first available prices\n",
    "      self.last_prices = self.data.loc[self.data.index.get_level_values(0).unique()[0]]['Open'].values\n",
    "      self.last_prices = np.array(self.last_prices)\n",
    "      # Initialize state\n",
    "      self._update_state()\n",
    "\n",
    "      return self.state, {}\n",
    "\n",
    "    def _update_state(self):\n",
    "        \"\"\"Update the current state.\"\"\"\n",
    "        current_date = self.data.index.get_level_values(0).unique()[self.current_step]\n",
    "\n",
    "        # Historical data\n",
    "        current_data = self.data.loc[current_date]\n",
    "        forward_returns = current_data['Forward_Return'].values\n",
    "        sentiment_scores = get_sentiment_scores([f\"News for {ticker}\" for ticker in self.tickers]) #placeholder\n",
    "        self.stock_prices = current_data['Open'].values\n",
    "        self.stock_prices = np.array(self.stock_prices)\n",
    "\n",
    "        hlc_state = np.concatenate([forward_returns, sentiment_scores])\n",
    "        llc_states = []\n",
    "        for i in range(self.num_stocks):\n",
    "          llc_states.append([self.stock_prices[i], self.holdings[i], self.balance])\n",
    "        llc_states = np.array(llc_states)\n",
    "\n",
    "        self.state = {\n",
    "            \"hlc_state\": hlc_state,\n",
    "            \"llc_state\": llc_states,\n",
    "            \"stock_prices\": self.stock_prices\n",
    "         }\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Take a step in the environment based on provided actions.\"\"\"\n",
    "        previous_portfolio_value = self.portfolio_value\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # 1. Get current price from data\n",
    "        if not done:\n",
    "          self._update_state()\n",
    "        else:\n",
    "          return self.state, 0, done, {}\n",
    "\n",
    "        # Scale actions\n",
    "        scaled_actions = np.clip(actions, -1, 1)\n",
    "        trades = np.zeros(self.num_stocks) # initialise an empty array\n",
    "        # 2. Calculate trades\n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "          price = self.stock_prices[i]\n",
    "          trade_amount = scaled_actions[i] * self.hmax\n",
    "          trade_amount = np.round(trade_amount) # make integer values\n",
    "          trades[i] = trade_amount\n",
    "          if trade_amount > 0: #Buy stocks\n",
    "              cost = price * trade_amount * (1 + self.buy_cost_pct)\n",
    "              if self.balance >= cost:\n",
    "                self.balance -= cost\n",
    "                self.holdings[i] += trade_amount\n",
    "              else: # can't afford this amount\n",
    "                trades[i] = 0\n",
    "          elif trade_amount < 0: #sell stocks\n",
    "              trade_amount = abs(trade_amount)\n",
    "              if self.holdings[i] >= trade_amount:\n",
    "                  proceeds = price * trade_amount * (1 - self.sell_cost_pct)\n",
    "                  self.balance += proceeds\n",
    "                  self.holdings[i] -= trade_amount\n",
    "              else:\n",
    "                 trades[i] = 0\n",
    "\n",
    "        # Update portfolio value (current total)\n",
    "        self.portfolio_value = self.balance + np.sum(self.holdings * self.stock_prices)\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = (self.portfolio_value - previous_portfolio_value) * self.reward_scaling\n",
    "        reward = np.clip(reward, -1, 1) #clip to help stabilise the training process\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "# 4. HLC (High-Level Controller)\n",
    "class HLC_PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(HLC_PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class HLC_ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(HLC_ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def ppo_update(hlc_policy, hlc_value, optimizer_policy, optimizer_value, states, actions, returns, advantages, clip_param=0.2, entropy_beta=0.01):\n",
    "    \"\"\"Standard PPO update step\"\"\"\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.float32).to(device)\n",
    "    returns = torch.tensor(np.array(returns), dtype=torch.float32).to(device)\n",
    "    advantages = torch.tensor(np.array(advantages), dtype=torch.float32).to(device)\n",
    "    #calculate new policy\n",
    "    new_actions = hlc_policy(states)\n",
    "    new_action_probs = torch.sigmoid(new_actions)\n",
    "\n",
    "    old_action_probs = torch.sigmoid(actions)\n",
    "\n",
    "    ratio = torch.exp(torch.log(new_action_probs) - torch.log(old_action_probs))\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_param, 1 + clip_param)\n",
    "    surrogate_objective = torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "\n",
    "    #entropy regularization - prevent policy from becoming deterministic\n",
    "    entropy = -(new_action_probs * torch.log(new_action_probs + 1e-8)).mean() # add a small number to avoid log of 0\n",
    "\n",
    "    #policy gradient update\n",
    "    policy_loss = - (surrogate_objective - entropy_beta * entropy)\n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_policy.step()\n",
    "\n",
    "    #calculate value loss\n",
    "    values = hlc_value(states).squeeze()\n",
    "    value_loss = ((values - returns)**2).mean()\n",
    "    optimizer_value.zero_grad()\n",
    "    value_loss.backward()\n",
    "    optimizer_value.step()\n",
    "\n",
    "def create_advantage_estimator(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
    "  \"\"\"Generalized advantage estimation\"\"\"\n",
    "  advantages = []\n",
    "  advantage = 0\n",
    "  for i in reversed(range(len(rewards))):\n",
    "    delta = rewards[i] + gamma * values[i+1] - values[i] if i < len(rewards) - 1 else rewards[i] - values[i]\n",
    "    advantage = delta + gamma * gae_lambda * advantage\n",
    "    advantages.insert(0, advantage) # insert at beginning so we have correct order for training\n",
    "  return advantages\n",
    "\n",
    "# 5. LLC (Low-Level Controller)\n",
    "class LLC_ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LLC_ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class LLC_CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(LLC_CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def ddpg_update(actor, critic, target_actor, target_critic, optimizer_actor, optimizer_critic, replay_buffer, batch_size, discount_factor=0.99, tau=0.005):\n",
    "    \"\"\"Standard DDPG update step\"\"\"\n",
    "    if len(replay_buffer) < batch_size: return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.float32).to(device)\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(np.array(dones), dtype=torch.float32).to(device)\n",
    "\n",
    "    target_actions = target_actor(next_states)\n",
    "    target_q_values = target_critic(next_states, target_actions).squeeze()\n",
    "    expected_q_values = rewards + (discount_factor * target_q_values * (1 - dones))\n",
    "\n",
    "    #critic loss\n",
    "    q_values = critic(states, actions).squeeze()\n",
    "    critic_loss = ((q_values - expected_q_values)**2).mean()\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "\n",
    "    #actor loss\n",
    "    policy_actions = actor(states)\n",
    "    actor_loss = -critic(states, policy_actions).mean()\n",
    "    optimizer_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "\n",
    "    #update target networks\n",
    "    for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "      target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "      target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "# 6. HRT Agent\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer.\"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "      batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "      states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in batch])\n",
    "      return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class HRTAgent:\n",
    "    def __init__(self, env, hlc_config, llc_config, device):\n",
    "        # Create High Level Controller (HLC)\n",
    "        self.num_stocks = env.num_stocks\n",
    "        self.hlc_policy = HLC_PolicyNetwork(hlc_config[\"input_size\"], hlc_config[\"hidden_size\"], hlc_config[\"output_size\"]).to(device)\n",
    "        self.hlc_value = HLC_ValueNetwork(hlc_config[\"input_size\"], hlc_config[\"hidden_size\"]).to(device)\n",
    "        self.hlc_optimizer_policy = optim.Adam(self.hlc_policy.parameters(), lr=hlc_config[\"learning_rate_policy\"])\n",
    "        self.hlc_optimizer_value = optim.Adam(self.hlc_value.parameters(), lr=hlc_config[\"learning_rate_value\"])\n",
    "        # Create Low Level Controller (LLC)\n",
    "        self.llc_actor = LLC_ActorNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_critic = LLC_CriticNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_target_actor = LLC_ActorNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_target_critic = LLC_CriticNetwork(llc_config[\"input_size\"], llc_config[\"hidden_size\"], llc_config[\"output_size\"]).to(device)\n",
    "        self.llc_optimizer_actor = optim.Adam(self.llc_actor.parameters(), lr=llc_config[\"learning_rate_actor\"])\n",
    "        self.llc_optimizer_critic = optim.Adam(self.llc_critic.parameters(), lr=llc_config[\"learning_rate_critic\"])\n",
    "        self.llc_replay_buffer = ReplayBuffer(llc_config[\"replay_buffer_size\"])\n",
    "        # Initialize Target Networks\n",
    "        self.llc_target_actor.load_state_dict(self.llc_actor.state_dict())\n",
    "        self.llc_target_critic.load_state_dict(self.llc_critic.state_dict())\n",
    "\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, state, exploration_noise=None):\n",
    "      with torch.no_grad():\n",
    "        hlc_state = torch.tensor(state[\"hlc_state\"], dtype=torch.float32).to(self.device)\n",
    "        hlc_action = self.hlc_policy(hlc_state).cpu().numpy()\n",
    "        llc_actions = []\n",
    "        for i in range(self.num_stocks):\n",
    "          if hlc_action[i] > 0.5: #buy action\n",
    "            llc_state = torch.tensor(state[\"llc_state\"][i], dtype=torch.float32).to(self.device)\n",
    "            action = self.llc_actor(llc_state)\n",
    "            if exploration_noise is not None:\n",
    "              noise = torch.normal(torch.zeros(action.shape), exploration_noise).to(self.device)\n",
    "              action = (action + noise).clamp(-1, 1)\n",
    "            llc_actions.append(action.cpu().numpy())\n",
    "          elif hlc_action[i] < -0.5: #sell action\n",
    "              llc_state = torch.tensor(state[\"llc_state\"][i], dtype=torch.float32).to(self.device)\n",
    "              action = self.llc_actor(llc_state)\n",
    "              if exploration_noise is not None:\n",
    "                noise = torch.normal(torch.zeros(action.shape), exploration_noise).to(self.device)\n",
    "                action = (action + noise).clamp(-1, 1)\n",
    "              llc_actions.append(action.cpu().numpy())\n",
    "          else: # hold action\n",
    "              llc_actions.append(np.array(0))\n",
    "\n",
    "        return hlc_action, llc_actions\n",
    "\n",
    "\n",
    "    def calculate_hlc_reward(self, last_state, next_state, hlc_action, llc_reward, at, price_changes):\n",
    "      \"\"\"Calculate alignment and overall reward\"\"\"\n",
    "      #calculate alignment reward\n",
    "      alignment_rewards = []\n",
    "      for i in range(self.num_stocks):\n",
    "        action_i = 1 if hlc_action[i] > 0.5 else -1 if hlc_action[i] < -0.5 else 0\n",
    "        price_change_sign = np.sign(price_changes[i])\n",
    "        alignment_reward = np.sign(action_i) * price_change_sign if action_i != 0 else 0\n",
    "        alignment_rewards.append(alignment_reward)\n",
    "      alignment_rewards = np.array(alignment_rewards)\n",
    "\n",
    "      return at * np.sum(alignment_rewards) + (1 - at) * llc_reward\n",
    "\n",
    "    def phased_alternating_train(self, num_episodes, env, training_config, writer, at_initial_value = 1, at_decay = 0.001):\n",
    "      \"\"\" Train using the phased alternating algorithm.\"\"\"\n",
    "\n",
    "      self.current_step = 0\n",
    "      for episode in tqdm.tqdm(range(num_episodes), desc=\"Training Progress\", colour=\"green\"):\n",
    "          at = at_initial_value * np.exp(-at_decay * episode)\n",
    "          # Phase 1: HLC Training\n",
    "          if episode < training_config[\"hlc_episodes\"]:\n",
    "              states, actions, rewards, log_probs, values = self.train_hlc(env, training_config, writer, at, hlc_only=True)\n",
    "              for step, reward in enumerate(rewards):\n",
    "                writer.add_scalar('Rewards/HLC_only', reward, self.current_step + step)\n",
    "              self.current_step += len(states)\n",
    "          # Phase 2: LLC Training\n",
    "          elif episode >= training_config[\"hlc_episodes\"] and episode < training_config[\"llc_episodes\"]:\n",
    "              self.train_llc(env, training_config, writer)\n",
    "          # Phase 3: Alternating Training\n",
    "          else:\n",
    "            states, actions, rewards, log_probs, values = self.train_hlc(env, training_config, writer, at, hlc_only=False)\n",
    "            for step, reward in enumerate(rewards):\n",
    "              writer.add_scalar('Rewards/Combined', reward, self.current_step + step)\n",
    "            self.current_step += len(states)\n",
    "            self.train_llc(env, training_config, writer)\n",
    "\n",
    "    def train_hlc(self, env, training_config, writer, at, hlc_only=False):\n",
    "      \"\"\"Train the HLC network\"\"\"\n",
    "      states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "\n",
    "      state = env.reset()[0] # get only the state from the reset\n",
    "      done = False\n",
    "      t = 0\n",
    "      while not done:\n",
    "        hlc_state = state[\"hlc_state\"]\n",
    "        llc_state = state[\"llc_state\"]\n",
    "        hlc_action, llc_actions = self.select_action(state, exploration_noise=None)\n",
    "        next_state, reward, done, _ = env.step(llc_actions)\n",
    "\n",
    "        price_changes = []\n",
    "        if t > 0: # calculate if there were price changes\n",
    "            for i in range(self.num_stocks):\n",
    "                price_changes.append(next_state[\"stock_prices\"][i] - state[\"stock_prices\"][i])\n",
    "        else: #set to 0 on first iteration\n",
    "            price_changes = np.zeros(self.num_stocks)\n",
    "\n",
    "        # calculate reward\n",
    "        if hlc_only:\n",
    "          r = self.calculate_hlc_reward(state, next_state, hlc_action, 0, at, price_changes)\n",
    "        else:\n",
    "          r = self.calculate_hlc_reward(state, next_state, hlc_action, reward, at, price_changes)\n",
    "        states.append(hlc_state)\n",
    "        actions.append(hlc_action)\n",
    "        values.append(self.hlc_value(torch.tensor(np.array(hlc_state), dtype=torch.float32).to(self.device)).detach().cpu().numpy())\n",
    "        rewards.append(r)\n",
    "        state = next_state\n",
    "        t += 1\n",
    "\n",
    "      values.append(self.hlc_value(torch.tensor(np.array(next_state[\"hlc_state\"]), dtype=torch.float32).to(self.device)).detach().cpu().numpy())\n",
    "      advantages = create_advantage_estimator(rewards, values, gamma=training_config[\"gamma\"], gae_lambda=training_config[\"gae_lambda\"])\n",
    "      for _ in range(training_config[\"ppo_epochs\"]):\n",
    "        ppo_update(self.hlc_policy, self.hlc_value, self.hlc_optimizer_policy, self.hlc_optimizer_value, states, actions, rewards, advantages)\n",
    "\n",
    "      return states, actions, rewards, log_probs, values\n",
    "\n",
    "    def train_llc(self, env, training_config, writer):\n",
    "        \"\"\"Train the LLC network\"\"\"\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "          hlc_state = state[\"hlc_state\"]\n",
    "          llc_state = state[\"llc_state\"]\n",
    "          hlc_action, llc_actions = self.select_action(state, exploration_noise=training_config[\"exploration_noise\"])\n",
    "          next_state, reward, done, _ = env.step(llc_actions)\n",
    "\n",
    "          # Store in replay buffer\n",
    "          for i in range(self.num_stocks):\n",
    "            if hlc_action[i] > 0.5 or hlc_action[i] < -0.5: #only add to buffer if we take action\n",
    "              self.llc_replay_buffer.push(llc_state[i], llc_actions[i], reward, next_state[\"llc_state\"][i], done)\n",
    "\n",
    "          ddpg_update(self.llc_actor, self.llc_critic, self.llc_target_actor, self.llc_target_critic, self.llc_optimizer_actor, self.llc_optimizer_critic, self.llc_replay_buffer, training_config[\"batch_size\"], discount_factor=training_config[\"gamma\"], tau=training_config[\"tau\"])\n",
    "\n",
    "          state = next_state\n",
    "\n",
    "        writer.add_scalar('Rewards/LLC', reward, self.current_step)\n",
    "\n",
    "# 7. Training and Testing\n",
    "def train_hrt_agent(agent, env, num_episodes, training_config, writer):\n",
    "  \"\"\"Train the HRT agent\"\"\"\n",
    "  agent.phased_alternating_train(num_episodes, env, training_config, writer)\n",
    "  return agent\n",
    "\n",
    "def test_hrt_agent(agent, env, num_episodes, writer):\n",
    "  \"\"\" Test the HRT agent and calculate performance metrics.\"\"\"\n",
    "  state = env.reset()[0]\n",
    "  done = False\n",
    "  total_reward = 0\n",
    "  trades = 0\n",
    "  while not done:\n",
    "    hlc_action, llc_actions = agent.select_action(state, exploration_noise=None)\n",
    "    next_state, reward, done, _ = env.step(llc_actions)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    trades +=1\n",
    "  writer.add_scalar('Results/Test_TotalReward', total_reward, 1)\n",
    "  writer.add_scalar('Results/Test_NumTrades', trades, 1)\n",
    "  print(f\"Test Result - Total reward: {total_reward}, Trades taken {trades}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is not allowed.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m end_date_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2021-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create the custom environment\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m env_train \u001b[38;5;241m=\u001b[39m \u001b[43mStockTradingEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_stocks_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m env_test \u001b[38;5;241m=\u001b[39m StockTradingEnv(all_stocks_data, start_date_test, end_date_test)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mStockTradingEnv.__init__\u001b[0;34m(self, dataframes, start_date, end_date, initial_amount, hmax, buy_cost_pct, sell_cost_pct, reward_scaling)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_date \u001b[38;5;241m=\u001b[39m start_date\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_date \u001b[38;5;241m=\u001b[39m end_date\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_stocks_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Define action space\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stocks,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(dataframes, start_date, end_date, days_forward)\u001b[0m\n\u001b[1;32m     45\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df[cols_to_convert \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     47\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m---> 48\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_forward_returns(x, days_forward\u001b[38;5;241m=\u001b[39mdays_forward))\n\u001b[1;32m     50\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Vandan_Personal_Folder/CODE_STUFF/Projects/venvs/mathematricks_venv/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Vandan_Personal_Folder/CODE_STUFF/Projects/venvs/mathematricks_venv/lib/python3.10/site-packages/pandas/core/indexing.py:1411\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/Vandan_Personal_Folder/CODE_STUFF/Projects/venvs/mathematricks_venv/lib/python3.10/site-packages/pandas/core/indexing.py:1443\u001b[0m, in \u001b[0;36m_LocIndexer._get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1442\u001b[0m labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m-> 1443\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/Vandan_Personal_Folder/CODE_STUFF/Projects/venvs/mathematricks_venv/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:697\u001b[0m, in \u001b[0;36mDatetimeIndex.slice_indexer\u001b[0;34m(self, start, end, step)\u001b[0m\n\u001b[1;32m    694\u001b[0m     in_index \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m (end_casted \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_index:\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue based partial slicing on non-monotonic DatetimeIndexes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-existing keys is not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m indexer \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m][::step]\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is not allowed.'"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "  # Load Data and setup the environment\n",
    "  data_folder = \"/Users/vandanchopra/Vandan_Personal_Folder/CODE_STUFF/Projects/mathematricks/db/data/ibkr/1d\"  # Replace with your data folder path\n",
    "  all_stocks_data = load_stock_data(data_folder)\n",
    "  start_date_train = '2015-01-01'\n",
    "  end_date_train = '2019-12-31'\n",
    "  start_date_test = '2021-01-01'\n",
    "  end_date_test = '2021-12-31'\n",
    "\n",
    "  # Create the custom environment\n",
    "  env_train = StockTradingEnv(all_stocks_data, start_date_train, end_date_train)\n",
    "  env_test = StockTradingEnv(all_stocks_data, start_date_test, end_date_test)\n",
    "\n",
    "  # Configuration\n",
    "  hlc_config = {\n",
    "    \"input_size\": env_train.observation_space[\"hlc_state\"].shape[0],  # Use the observation space of your specific environment.\n",
    "    \"hidden_size\": 128,\n",
    "    \"output_size\": len(all_stocks_data), # buy/sell/hold for each stock\n",
    "    \"learning_rate_policy\": 3e-4,\n",
    "    \"learning_rate_value\": 1e-3\n",
    "  }\n",
    "  llc_config = {\n",
    "    \"input_size\": env_train.observation_space[\"llc_state\"].shape[2], # observation for each stock\n",
    "    \"hidden_size\": 128,\n",
    "    \"output_size\": 1,  # trading volume for each stock\n",
    "    \"learning_rate_actor\": 1e-3,\n",
    "    \"learning_rate_critic\": 1e-3,\n",
    "    \"replay_buffer_size\": 200000\n",
    "  }\n",
    "\n",
    "  training_config = {\n",
    "    \"hlc_episodes\": 10,\n",
    "    \"llc_episodes\": 20,\n",
    "    \"ppo_epochs\": 5,\n",
    "    \"exploration_noise\": 0.1,\n",
    "    \"batch_size\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"tau\": 0.005\n",
    "  }\n",
    "\n",
    "  num_training_episodes = 30 # Adjust as needed\n",
    "  writer = SummaryWriter()\n",
    "  # Create and Train HRT Agent\n",
    "  hrt_agent = HRTAgent(env_train, hlc_config, llc_config, device)\n",
    "  trained_agent = train_hrt_agent(hrt_agent, env_train, num_training_episodes, training_config, writer)\n",
    "  # Test the agent\n",
    "  test_hrt_agent(trained_agent, env_test, num_training_episodes, writer)\n",
    "\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathematricks_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
